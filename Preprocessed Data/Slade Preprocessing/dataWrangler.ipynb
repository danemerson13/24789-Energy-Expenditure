{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataWrangler.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNHDckbIfOJ+17plUnDpstB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCevdSl6Zq7o"},"outputs":[],"source":["import sys\n","from glob import glob\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from scipy.signal import filtfilt, butter\n","from scipy import signal\n","from numpy import genfromtxt\n","from natsort import natsorted\n","import os\n","from shutil import copy\n","import csv\n","import datetime\n","import pytz\n","cwd = os.getcwd()\n","\n","def loadTrueMetInterp(met_ss_interp):\n","    \n","    met_ss_interp_avg = np.mean(met_ss_interp,axis=0) # should be just size 5 for standing, walk, run\n","    met_ss_speeds = np.array([0.,1.,1.5,2.5,3.])#,3.5])\n","    #dt_met = 2*met_ss_interp_avg[-1] - met_ss_interp_avg[-2]\n","    #met_ss_interp_avg = np.append(met_ss_interp_avg, np.array(dt_met))\n","    \n","    tv1_sp = np.array([0.0, 0.0, 1.0, 1.0, 0.0])#[0.0, 1.0, 1.0, 0.0, 0.0])\n","    tv1_t = np.array([0., 4., 6., 16., 18.])#[0., 1., 12., 13., 24.])\n","    tv1_int = np.arange(0,24)\n","    tv1_true_sp = np.interp(tv1_int,tv1_t,tv1_sp)\n","    tv1_true_met = np.interp(tv1_true_sp, met_ss_speeds, met_ss_interp_avg)\n","    \n","    tv4_sp = np.array([1.0, 1.0, 3.0, 3.0, 1.0])#[1.0, 3.0, 3.0, 1.0, 1.0])\n","    tv4_t = np.array([0., 6., 10., 22., 26.])#[0., 4., 15., 19., 30.])\n","    tv_int = np.arange(0,30)\n","    tv4_true_sp = np.interp(tv_int,tv4_t,tv4_sp)\n","    tv4_true_met = np.interp(tv4_true_sp, met_ss_speeds, met_ss_interp_avg)  \n","    \n","    # add sine function for 2,3\n","    p = 30.0\n","    t = np.linspace(0,29,30)\n","    min_speed = 1.0\n","    max_speed = 1.5\n","    avg = (min_speed + max_speed)/2.0\n","    ampl = (max_speed - min_speed)/2.0\n","    tv_sine = avg + ampl*np.sin(2*3.14159*(t-p/4-6)/p)\n","    tv2_true_met = np.interp(tv_sine, met_ss_speeds, met_ss_interp_avg)\n","    \n","    min_speed = 1.0\n","    max_speed = 3.00\n","    avg = (min_speed + max_speed)/2.0\n","    ampl = (max_speed - min_speed)/2.0\n","    tv_sine = avg + ampl*np.sin(2*3.14159*(t-p/4-6)/p)\n","    tv3_true_met = np.interp(tv_sine, met_ss_speeds, met_ss_interp_avg)\n","    \n","    tv_mat = np.zeros((4,30))\n","    tv_mat[0,:24] = tv1_true_met\n","    tv_mat[1,:] = tv2_true_met\n","    tv_mat[3,:] = tv3_true_met\n","    tv_mat[2,:] = tv4_true_met\n","    \n","    return tv_mat\n","\n","def loadRawMet(data_dir, subj, timezone):\n","    gen_files = os.listdir(data_dir+subj)\n","    for filename in gen_files:\n","        if len(filename) >= 4:\n","            if filename[-4:] == 'xlsx': # metabolics file\n","                met_array = pd.read_excel(data_dir+subj+'\\\\'+filename, names=[\"time\",\"V02\",\"VC02\",\"HR\"], skiprows=3, usecols=[9, 14, 15, 23]) # left then right insole forces   \n","                met_array['MET'] = (met_array[\"V02\"]*16.48 + met_array[\"VC02\"]*4.48)/60.\n","                dt = [i.hour*3600 + i.minute*60 + i.second for i in met_array[\"time\"]]\n","\n","                # load starting time\n","                met_raw = np.array(pd.read_excel(data_dir+subj+'\\\\'+filename, usecols=[4], header=None))\n","                starting_time = met_raw[0:2]\n","                test = str(starting_time[0][0]) + ' ' + str(starting_time[1][0])\n","                test_dt = datetime.datetime.strptime(test, '%m/%d/%Y %I:%M:%S %p')  \n","                test_dt = timezone.localize(test_dt)\n","                time_stamps = [test_dt + datetime.timedelta(seconds=i) for i in dt]\n","                met_array[\"time_stamp\"] = time_stamps\n","                met_array[\"dt\"] = np.array(dt) - dt[0] \n","                \n","    return met_array, time_stamps\n","\n","# gets all data between start and stop time \n","def getCondData(input_data, time_stamps, start_time, cond_len, time_stamp_label='time_stamp', tz = \"America/Los_Angeles\"):\n","    stop_time = start_time + datetime.timedelta(seconds=cond_len)\n","    mask = (input_data[time_stamp_label] > start_time) & (input_data[time_stamp_label] < stop_time)\n","    try:\n","        last_true_ind = mask.index[mask][-1]\n","        mask[last_true_ind+1] = True\n","    except:\n","        print(\"error adding last index beyond the end of the file\")\n","    input_subset = input_data.loc[mask]\n","    first_true_ind = mask.index[mask][0]\n","    input_subset[\"dt\"] = np.array(input_subset[\"dt\"]) - np.array(input_subset[\"dt\"])[0] + (time_stamps[first_true_ind] - start_time).total_seconds()\n","    return input_subset\n","\n","# input is list of times, iterate through and output a list of datetime objects\n","def convertAppleTime2DateTime(time_list):\n","    dt_list = []\n","    for i, time_str in enumerate(time_list):\n","        dt_str_split = time_str.split()\n","        dt1_date = datetime.datetime.strptime(dt_str_split[0]+\" \"+dt_str_split[1], \"%Y-%m-%d %H:%M:%S\")\n","        dt_list.append(dt1_date)\n","    return dt_list\n","\n","# gets all data between start and stop time \n","def getDataBetweenStartStop(input_data, start_time, stop_time, tz = \"America/Los_Angeles\"):\n","    mask = (input_data['startDate'] > start_time) & (input_data['endDate'] < stop_time)\n","    try:\n","        last_true_ind = mask.index[mask][-1]\n","        mask[last_true_ind+1] = True\n","    except:\n","        print(\"error adding last index beyond the end of the file\")\n","    input_subset = input_data.loc[mask]\n","    input_subset['cumTime'] -= input_subset.iloc[0]['cumTime']\n","    return input_subset\n","\n","# load the main data and pre-process times, and compute EE\n","def loadRawWatchData(watch_data_dir, file_type, kcal2watt, tz=\"America/Los_Angeles\"):\n","    load_mat = pd.read_csv(watch_data_dir+file_type)\n","    load_mat['startDate'] = pd.to_datetime(load_mat['startDate'])\n","    load_mat['endDate'] = pd.to_datetime(load_mat['endDate'])\n","    load_mat['startDate'] = load_mat['startDate'].dt.tz_convert(tz)\n","    load_mat['endDate'] = load_mat['endDate'].dt.tz_convert(tz)\n","    # compute time interval and EE from these datetime vectors\n","    dt_vec_raw = load_mat['endDate'] - load_mat['startDate']\n","    dt_vec = [i.total_seconds() for i in dt_vec_raw]\n","    dt_fs_vec_raw = load_mat['startDate'] - load_mat.loc[0]['startDate']\n","    dt_cum = [i.total_seconds() for i in dt_fs_vec_raw]\n","    for i, ele in enumerate(dt_vec):\n","        if ele == 0: # if time stamps between energy expenditure output is 0, then make it 1 second\n","            dt_vec[i] = 1.0\n","    ee_vec = load_mat['value']*kcal2watt/(np.array(dt_vec)/3600.0)\n","    load_mat['time'] = dt_vec\n","    load_mat['cumTime'] = dt_cum\n","    load_mat['ee'] = ee_vec\n","    return load_mat\n","\n","# take processed subsets of data and compute ee per second\n","# return array of [time (s), ee]\n","def interpolateWatchData(input_data, start_time, hr = False, cond_len_s=300, kcal2watt=1.16279):\n","    interp_vec = -1*np.ones(cond_len_s)\n","    skip_inds = []\n","    for i, ele in input_data.iterrows():\n","        if len(skip_inds) > 0:\n","            if len(skip_inds) == 1:\n","                skip_inds = []\n","            else:\n","                skip_inds = skip_inds[1:]\n","            continue\n","        dt = ele['startDate'] - start_time\n","        start_idx = dt.total_seconds()\n","        stop_idx = (ele['endDate'] - start_time).total_seconds()\n","        if start_idx == stop_idx:\n","            stop_idx += 1\n","        idx_rng = np.arange(np.maximum(int(start_idx),0), np.minimum(int(stop_idx), cond_len_s))\n","        # check if future indeces have the same value and lump them all together\n","        if (len(idx_rng) > 0) and not hr: # some data is within the range and compute EE\n","            new_ind_cnt = 1\n","            check_next_pt = True\n","            while(check_next_pt): # if next data point is in the \n","                next_val_ind = i+new_ind_cnt\n","                if next_val_ind > input_data.last_valid_index():\n","                    break #break_flag = True\n","                else:\n","                    try:\n","                        next_val = input_data['value'].loc[next_val_ind]\n","                    except:\n","                        print('Past max \"reachable\" index... ', input_data.last_valid_index(), next_val_ind)\n","                        break\n","                if (ele['value'] == next_val): # they are the same\n","                    skip_inds.append(next_val_ind)\n","                    new_ind_cnt += 1\n","                else: # next index is different\n","                    check_next_pt = False\n","                    if len(skip_inds) > 0: # some data points to append to idx_rng\n","                        stop_idx = (input_data['endDate'].loc[next_val_ind-1] - start_time).total_seconds()\n","                        idx_rng = np.arange(np.maximum(int(start_idx),0), np.minimum(int(stop_idx), cond_len_s))\n","                        # compute new EE estimate rather than from each timestamp\n","                        new_dt = (input_data['endDate'].loc[next_val_ind-1] - ele['startDate']).total_seconds()\n","                        if new_dt == 0:\n","                            print(\"0 in the new_dt\")\n","                            new_dt = 1\n","                        new_ee = ele['value']*kcal2watt*(new_ind_cnt)/(new_dt/3600.0)\n","                        interp_vec[idx_rng] = new_ee\n","                    else:\n","                        interp_vec[idx_rng] = ele['ee']\n","        elif hr:\n","            interp_vec[idx_rng] = ele['value']\n","        else:\n","            interp_vec[idx_rng] = ele['ee']\n","    # then correct for any missing seconds by applying the previous ee value that is not -1\n","    for i, ele in enumerate(interp_vec):\n","        if ele == -1:\n","            index = np.where(interp_vec != -1.0)\n","            if i == 0:\n","                if len(index[0]) == 0:\n","                    #print(\"No watch measurements found for this condition. Taking initial value.\")\n","                    if hr:\n","                        interp_vec = input_data['value'].iloc[0]*np.ones(cond_len_s)\n","                    else:\n","                        interp_vec = input_data['ee'].iloc[0]*np.ones(cond_len_s)\n","                    break\n","                else:\n","                    interp_vec[i] = interp_vec[index[0][0]]\n","            else:\n","                interp_vec[i] = interp_vec[i-1]\n","    return interp_vec\n","\n","# combine interpolated active and basal energy expenditure\n","def combineActiveBasal(active_e, passive_e, start_time, hr = False, cond_len_s = 300):\n","    active_interp = interpolateWatchData(active_e, start_time, hr, cond_len_s)\n","    passive_interp = interpolateWatchData(passive_e, start_time, hr, cond_len_s)\n","    return active_interp + passive_interp\n","\n","def watchValidationPlot(ee_interp, hr_interp, ee_met, hr, dd):\n","    fig, ax1 = plt.subplots()\n","    plt.title('Apple watch data')\n","    ax1.set_xlabel('Time (s)')\n","    ax1.set_ylabel('Energy expenditure (W)')\n","    ax1.plot(np.arange(len(ee_interp)),ee_interp, ls = '--', color = 'k', label='Watch EE estimate')\n","    ax1.plot(np.arange(len(ee_met)), ee_met, color='k', alpha=0.7, label='Respirometry per breath')\n","    ax1.plot(np.arange(len(dd)), dd, color='b', label='Data driven')\n","    ax1.legend(loc='lower right')\n","    ax2 = ax1.twinx()\n","    color = 'tab:red'\n","    ax2.plot(np.arange(len(hr_interp)),hr_interp, color = color, ls = '--', label='Watch heart rate')\n","    ax2.plot(np.arange(len(hr)),hr, color = color, label='Heart rate monitor')\n","    ax2.set_ylabel('Heart rate (bpm)', color = color)\n","    ax2.tick_params(axis='y',labelcolor = color)\n","    ax2.legend(loc='lower left')\n","    fig.tight_layout()\n","    plt.show()\n","\n","# takes in input data in [time steps x feats] and computes heel strikes over sliding window\n","# returns processed matrix of processed gait cycles, stacked [gaits x binned feats]\n","def simRealStrikes(input_data, weight, height, shift_ind, stride_detect_window, detect_window, peak_height_thresh, peak_min_dist, shank_gyro_z_ind, b, a, deg2rad, old_data = False, data_rate = 100.0):\n","    gait_data = []\n","    t_steps, feats = input_data.shape\n","    watching_heelstrike = True\n","    watch_strike_cnt = 0 \n","    time_list = []\n","    time_of_gait = []\n","    for k in range(t_steps - stride_detect_window):\n","        stride_window = input_data[k:k+stride_detect_window]\n","        if watching_heelstrike: # if looking for heelstrike\n","            peak_list = checkPeaks(stride_window[:,shank_gyro_z_ind], b, a, peak_height_thresh, peak_min_dist, deg2rad, old_data) # check for peaks\n","            if len(peak_list) > 1: # checking if a new heel strike has occured\n","                if (stride_detect_window - peak_list[-1]) < detect_window: # peak has occured in last detect_window of data\n","                    watching_heelstrike = False # now wait a bit to detect heelstrikes again\n","                    new_gait_cycle = processRawGait(stride_window, peak_list[-2], peak_list[-1], shift_ind, b, a, weight, height, deg2rad, old_data) # process most recent gait data\n","                    #time_stamp = round(time.time() - init_time,3) add later by sample cnt?\n","                    new_gait_cycle = np.expand_dims(new_gait_cycle, axis=0)\n","                    time_list.append(peak_list[-1] - peak_list[-2])\n","                    time_of_gait.append((k + peak_list[-1])/data_rate)\n","                    if len(gait_data) == 0:\n","                        gait_data = new_gait_cycle\n","                    else:\n","                        gait_data = np.concatenate((gait_data, new_gait_cycle), axis=0)\n","                    \n","        else: # count until the peak has cleared the recent window\n","            if watch_strike_cnt > detect_window:\n","                watching_heelstrike = True\n","                watch_strike_cnt = 0\n","            else:\n","                watch_strike_cnt += 1        \n","    return gait_data, time_list, time_of_gait\n","\n","# for saved IMU data, make it into the same format as the real-time estimates\n","def computeEstimatesFromIMU(real_time_est, subj, cond, timezone, estimate_file_name, mass, height, shift_ind, stride_detect_window, detect_window, peak_height_thresh, peak_min_dist, shank_gyro_z_ind, b, a, deg2rad, model_weights, basal_rate, file_len_in_s = 5.0):\n","    subj_cond_dir = real_time_est + subj + '\\\\' + cond + '\\\\'\n","    basal_rate = round(float(basal_rate), 3)\n","    data_files = os.listdir(subj_cond_dir)\n","    data_files = natsorted(data_files)\n","    sample_data = np.array([])\n","    with open(subj_cond_dir+data_files[-1], 'r') as f:\n","        reader = csv.reader(f, delimiter=',')\n","        for row in reader:\n","            init_ts = datetime.datetime.strptime(row[0],\"%d/%m/%Y %H:%M:%S\")\n","            break\n","    f.close()\n","    init_ts = timezone.localize(init_ts)\n","    for l, file in enumerate(data_files[:-1]):\n","        if (file == estimate_file_name) or (file[0:2] == 'TV'): # skip this file\n","            continue\n","        else:\n","            file_data = np.load(subj_cond_dir+file)\n","            if len(sample_data) == 0:\n","                sample_data = file_data\n","            else:\n","                sample_data = np.concatenate((sample_data, file_data), axis=0)\n","    gait_cycles,_,time_of_gait = simRealStrikes(sample_data[:,:-1], mass, height, shift_ind, stride_detect_window, detect_window, peak_height_thresh, peak_min_dist, shank_gyro_z_ind, b, a, deg2rad) \n","    num_gaits,_ = gait_cycles.shape\n","    gait_cycles = np.concatenate((np.ones((num_gaits,1)), gait_cycles), 1)\n","    estimates = np.round(np.dot(gait_cycles,model_weights),3)\n","    dt_stamps = [init_ts + datetime.timedelta(seconds=(i-file_len_in_s)) for i in time_of_gait]\n","    basal_t_thresh = 8.0\n","    offset_t = 1.0\n","    with open(subj_cond_dir+estimate_file_name,'w') as f:\n","        for p in range(num_gaits):\n","            if (p > 0) and ((dt_stamps[p]-dt_stamps[p-1]).total_seconds() >= basal_t_thresh): # large enough gap detected\n","                # adding first basal point after last gait\n","                f.write(\"{},{},{},{}\".format(time_of_gait[p-1]+offset_t, dt_stamps[p-1] + datetime.timedelta(seconds=offset_t), basal_rate, basal_rate))\n","                f.write(\"\\n\")                \n","                f.write(\"{},{},{},{}\".format(time_of_gait[p]-offset_t, dt_stamps[p]- datetime.timedelta(seconds=offset_t), basal_rate, basal_rate))\n","                f.write(\"\\n\")\n","            if estimates[p] < basal_rate:\n","                f.write(\"{},{},{},{}\".format(time_of_gait[p], dt_stamps[p], basal_rate, basal_rate))\n","            else:\n","                f.write(\"{},{},{},{}\".format(time_of_gait[p], dt_stamps[p], estimates[p], estimates[p]))\n","            f.write(\"\\n\")\n","    f.close()    \n","    \n","### Computing first order estimates for watch, hr, metabolics\n","def computeFirstOrderEstimates(t_mat, subjects, conditions, metabolics_real, met_mat, watch_int, hr_int, fo_time, watch_fo_int, hr_fo_int):\n","    # convert t_mat to all be in seconds\n","    for i in range(len(t_mat)):\n","        for j in range(len(t_mat[i])):\n","            offset = 3600*t_mat[i][j][0].hour + 60*t_mat[i][j][0].minute + t_mat[i][j][0].second\n","            for k in range(len(t_mat[i][j])):\n","                t_mat[i][j][k] = 3600*t_mat[i][j][k].hour + 60*t_mat[i][j][k].minute + t_mat[i][j][k].second - offset + 1\n","\n","    conditions = conditions[1:]\n","    # compute first order estimates of met, hr, watch\n","    for i, subj in enumerate(subjects):\n","        for j, cond in enumerate(conditions):\n","            # for watch add pt whenever there is a change in the estimate\n","            watch_est_list = [watch_int[i,j,0]]\n","            tvec = [1]\n","            for k in range(1,fo_time+2):\n","                if watch_int[i,j,k] != watch_est_list[-1]: # next point different\n","                    watch_est_list.append(watch_int[i,j,k])\n","                    tvec.append(k+1)\n","                if k >= 2:\n","                    watch_fo_int[i,j,k-2],_,_ = metabolic_rate_estimation(tvec, watch_est_list)\n","            # for hr add a pt for each interp value\n","            for k in range(2,fo_time+2):\n","                tvec = np.arange(1,k+1)\n","                hr_fo_int[i,j,k-2],_,_ = metabolic_rate_estimation(tvec, hr_int[i,j,:k])\n","    # for met do it like previous method for each breath\n","    tmin = 2\n","    tmax = 121\n","    t = np.arange(tmin,tmax)\n","    met_fo_int = np.zeros(len(t))\n","    met_ordering = np.zeros(len(t))\n","    met_est_full = np.zeros((len(subjects),len(conditions),tmax-tmin))\n","    for z,time_horizon in enumerate(t):\n","        metabolics_est = np.zeros((len(subjects),len(conditions)))\n","        for i in range(len(t_mat)):\n","            for j in range(1,len(t_mat[i])):\n","                end_ind = 0\n","                for k in range(len(t_mat[i][j])):\n","                    if t_mat[i][j][k] >= time_horizon: # save index and break\n","                        end_ind = k\n","                        break\n","                metabolics_est[i,j-1], y_bar, mean_squared_err = metabolic_rate_estimation(t_mat[i][j][:end_ind], met_mat[i][j][:end_ind])\n","        met_est_full[:,:,z] = metabolics_est\n","        met_fo_int[z], met_ordering[z] = compute_2min_met_errors(metabolics_real, metabolics_est)\n","\n","    return hr_fo_int, met_fo_int, watch_fo_int, met_est_full\n","    \n","# compute data-driven estimates interpolated at 1-second intervals\n","def computeDDinter(real_time_est, est_col_ind, subj, cond, subj_cond_dir, estimate_file_name, timezone, loc_cond_timestamp, cond_time_s, basal_flag=False, basal_rate = 0.0):\n","    basal_t_thresh = 8.0 # number of second gap between gait cycles to estimate adjusted standing rate\n","    subjcond_time = []\n","    subjcond_dt = []\n","    subjcond_est = []\n","    with open(subj_cond_dir+estimate_file_name, 'r') as f:\n","        reader = csv.reader(f, delimiter=',')\n","        for row in reader:\n","            if len(row[1]) == 25:\n","                new_dt = timezone.localize(datetime.datetime.strptime(row[1][:-6],\"%Y-%m-%d %H:%M:%S\")) #.%f-%Z\"))\n","            elif len(row[1]) == 26:\n","                #print(len(row[1]), row[1][:-7])\n","                new_dt = timezone.localize(datetime.datetime.strptime(row[1][:-7],\"%Y-%m-%d %H:%M:%S\")) #.%f-%Z\"))\n","            else:\n","                new_dt = timezone.localize(datetime.datetime.strptime(row[1][:-13],\"%Y-%m-%d %H:%M:%S\")) #.%f-%Z\"))\n","            subjcond_time.append(float(row[0]))\n","            subjcond_dt.append(new_dt)\n","            subjcond_est.append(float(row[est_col_ind]))\n","    f.close()\n","    # compute time difference between start datetime and first gait cycle\n","    time_diff = (loc_cond_timestamp - subjcond_dt[0]).total_seconds()\n","    # from time vector subjtract initial gait value and time difference\n","    subjcond_time = np.array(subjcond_time)\n","    subjcond_est = np.array(subjcond_est)\n","    subjcond_time = subjcond_time - (time_diff + subjcond_time[0])\n","    # find indeces of gaits occuring after the start of the condition (time == 0) and less than end of condition (time == 300)\n","    prev_gaits = subjcond_time >= 0.0\n","    subjcond_time = subjcond_time[prev_gaits]\n","    subjcond_est = subjcond_est[prev_gaits]\n","    prev_gaits2 = subjcond_time < cond_time_s\n","    subjcond_time = subjcond_time[prev_gaits2]\n","    subjcond_est = subjcond_est[prev_gaits2]\n","    tmin = 0\n","    tmax = cond_time_s\n","    t = np.arange(tmin,tmax)\n","    dd_int = np.interp(t, subjcond_time, subjcond_est)\n","    return dd_int, subjcond_est, subjcond_time\n","    \n","# take in shank IMU vector, filter, look for thresholding & min_distance, return strike indeces\n","def checkPeaks(strike_vec, b, a, peak_height_thresh, peak_min_dist, deg2rad, old_data = False):\n","    if old_data:\n","        strike_vec_filt = signal.filtfilt(b,a,strike_vec*(-1.0/deg2rad))\n","    else:\n","        strike_vec_filt = signal.filtfilt(b,a,strike_vec)\n","    peak_list = signal.find_peaks(strike_vec_filt, height=peak_height_thresh, distance=peak_min_dist)\n","    return peak_list[0]\n","    \n","# takes data and moves the first half of the data to the second half\n","# assumes data_array is size [time steps x feature]\n","def convertBetweenLegs(data_array):\n","    array_copy = np.copy(data_array)\n","    convert_array = np.zeros(data_array.shape)\n","    gait_time_steps,_ = data_array.shape\n","    half_gait = int(gait_time_steps/2)\n","    convert_array[:half_gait,:] = array_copy[half_gait:,:]\n","    convert_array[half_gait:,:] = array_copy[:half_gait,:]\n","    return convert_array\n","    \n","# downsample the data into a discrete number of bins\n","def binData(data_array, num_bins=30):\n","    return signal.resample(data_array, num_bins) # resamples along axis = 0 by default\n","\n","# shift the data in the binned matrix to the left by the given shift_ind\n","def shiftBinData(data_array, shift_ind, num_bins=30):\n","    array_copy = np.copy(data_array)\n","    convert_array = np.zeros(data_array.shape)\n","    convert_array[:-shift_ind, :] = array_copy[shift_ind:,:]\n","    convert_array[-shift_ind:, :] = array_copy[:shift_ind,:]\n","    return convert_array\n","\n","# pass in array of data to process [time_samples x num_features] into [num_bins x num_features]\n","# start_ind, end_ind are indeces of start/end of gait cycle\n","# shift_ind is correction term for timing diff between heel strike and IMU thresh\n","# b, a are filter parameters\n","def processRawGait(data_array, start_ind, end_ind, shift_ind, b, a, weight, height, deg2rad, old_data = False, num_bins=30):\n","    gait_data = data_array[start_ind:end_ind, :] # crop to the gait cycle\n","    if not old_data:\n","        gait_data = gait_data*np.array([deg2rad,-deg2rad,-deg2rad,deg2rad,-deg2rad,-deg2rad,1,-1,-1,1,-1,-1]) # flip y & z, convert to rad/s\n","    filt_gait_data = signal.filtfilt(b,a,gait_data, axis=0) # low-pass filter\n","    bin_gait = binData(filt_gait_data) # discretize data into bins\n","    shift_flip_bin_gait = bin_gait.transpose() # get in shape of [feats x bins] for correct flattening\n","    model_input = shift_flip_bin_gait.flatten()\n","    model_input = np.insert(model_input, 0, [weight, height]) # adding a 1 for the bias term at start\n","    return model_input\n","\n","# vec1 here is true vec, vec 2 is the estimate\n","def pairwise_similarity(vec1, vec2, threshold = 0.042):\n","    # figure out how many pairs without repeats\n","    pairwise_len = 0\n","    for i in range(0,len(vec1)-1):\n","        pairwise_len += (i+1)\n","    # initialize pairways datastructures    \n","    pairwise_est = np.zeros(pairwise_len)\n","    pairwise_real = np.zeros(pairwise_len)\n","    # go through pairs to determine if the first entry is less than (0), within the threshold (1), or greater than (2)\n","    # the second entry. \n","    counter = 0\n","    for i in range(len(vec1)-1):\n","        for j in range(i+1, len(vec1)):\n","            if mapd(vec1[i],vec1[j]) <= threshold: # they are equal, set element to 1\n","                pairwise_real[counter] = 1\n","            elif vec1[i] < vec1[j]:\n","                pairwise_real[counter] = 0\n","            else:\n","                pairwise_real[counter] = 2\n","                \n","            if mapd(vec2[i],vec2[j]) <= threshold: # they are equal, set element to 1\n","                pairwise_est[counter] = 1\n","            elif vec2[i] < vec2[j]:\n","                pairwise_est[counter] = 0\n","            else:\n","                pairwise_est[counter] = 2\n","                \n","            counter += 1\n","    # sum the number of same elements between real and est\n","    match = np.equal(pairwise_est, pairwise_real)\n","    correct = sum(match)\n","    return correct, correct, pairwise_len\n","\n","def mapd(vec1, vec2): # mean absolute percent difference\n","    return np.abs(vec1 - vec2) / ((vec1 + vec2)/2)\n","\n","def compute_2min_met_errors(metabolics_real, met_2min_est):\n","    mae_mat = (np.abs(metabolics_real-met_2min_est)/metabolics_real)\n","    met_mape = np.mean(mae_mat)\n","    subjs_acc = np.zeros(metabolics_real.shape[0])\n","    for i in range(metabolics_real.shape[0]): # loop thru subjects\n","        correct, correct, pairwise_len = pairwise_similarity(metabolics_real[i,:], met_2min_est[i,:])\n","        subjs_acc[i] = correct/pairwise_len        \n","    ordering = np.mean(subjs_acc)\n","    return met_mape, ordering\n","\n","def compute_2min_met_errors_subj(metabolics_real, met_2min_est):\n","    mae_mat = (np.abs(metabolics_real-met_2min_est)/metabolics_real)\n","    met_mape = np.mean(mae_mat)\n","    subjs_acc = np.zeros(metabolics_real.shape[0])\n","    for i in range(metabolics_real.shape[0]): # loop thru subjects\n","        correct, correct, pairwise_len = pairwise_similarity(metabolics_real[i,:], met_2min_est[i,:])\n","        subjs_acc[i] = correct/pairwise_len        \n","        \n","    ordering = np.mean(subjs_acc)\n","    return met_mape, ordering, subjs_acc \n","\n","def metabolic_rate_estimation(t, y_meas, tau=42):\n","    n_samp = len(t)\n","    A = np.zeros((n_samp,2))\n","    A[0,:] = [1,0]\n","    for i in range(1,n_samp):\n","        for j in range(2):\n","            dt = t[i] - t[i-1]\n","            if j == 0:\n","                A[i,j] = A[i-1,j]*(1-dt/tau)\n","            else:\n","                A[i,j] = A[i-1,j]*(1-dt/tau) + (dt/tau)\n","    x_star = np.dot(np.linalg.pinv(A),y_meas)\n","    y_bar = np.dot(A,x_star)\n","    mean_squared_err = np.dot(np.transpose(y_bar-y_meas),(y_bar-y_meas))/n_samp\n","    met_est = x_star[1]\n","    \n","    return met_est, y_bar, mean_squared_err\n","\n","def print_error_maps(debug_values):\n","    num_plots, num_subj, num_cond = debug_values.shape\n","    title_list = ['corr_max_angles', 'corr_diff_angles', 'mape_angles', 'corr_max_force', 'corr_diff_force', 'mape_force', 'zero_counter']\n","    for i in range(num_plots):\n","        plt.figure()\n","        plt.matshow(debug_values[i,:,:])\n","        plt.xlabel('Conditions')\n","        plt.ylabel('Subjects')\n","        plt.title(title_list[i])\n","        plt.colorbar()\n","        plt.show()\n","    # printing final values\n","    print(\"Average MAPE for forces: \", np.mean(np.mean(debug_values[5,:,:])))\n","    print(\"Average MAPE across angles: \", np.mean(np.mean(debug_values[2,:,:])))\n","\n","# a is the longer time series on both ends\n","# b is shorter on both sides\n","# returns a,b vectors overlapping and corr_ind of a\n","def trim_corr(a,b):\n","    corr = np.correlate(a,b,'valid')\n","    corr_ind = np.argmax(corr)\n","    new_a = a[corr_ind:(len(b)+corr_ind)]\n","    return new_a, b, corr_ind\n","    \n","# a is the longer time series on both ends\n","# b is shorter on both sides\n","# returns a,b vectors overlapping and corr_ind of a\n","def trim_corr_arrays(a,b,a_ind,b_ind,mean=True):\n","    if mean:\n","        corr = np.correlate(a[:,a_ind]-np.mean(a[:,a_ind]),b[:,b_ind]-np.mean(b[:,b_ind]),'valid')\n","    else:\n","        corr = np.correlate(a[:,a_ind],b[:,b_ind],'valid')\n","    corr_ind = np.argmax(corr)\n","    new_a = a[corr_ind:(len(b)+corr_ind),:]\n","    range_new_a = float(np.abs(np.max(new_a[:,a_ind]) - np.min(new_a[:,a_ind]))/2.0)\n","    corr_max = max(corr)/(1.0*len(new_a)*range_new_a**2)\n","    return new_a, b, corr_ind, corr_max, corr\n","\n","# pull metabolics from the metabolics .csv file and the subject height/weight from the other .csv\n","def calc_metabolics(data_dir, subj, visualize = False, add_heartrate = False, cond_len=14, len_cond_s = 300, end_time_len = 180):\n","    gen_files = os.listdir(data_dir+subj)\n","    t_mat = []\n","    met_mat = []\n","    hr_mat = []\n","    for filename in gen_files:\n","        if len(filename) >= 4:\n","            if filename[-4:] == 'xlsx': # metabolics file\n","                if add_heartrate:\n","                    met_array = np.array(pd.read_excel(data_dir+subj+'\\\\'+filename, skiprows=3, usecols=[9, 14, 15, 23, 37])) # left then right insole forces          \n","                else:\n","                    met_array = np.array(pd.read_excel(data_dir+subj+'\\\\'+filename, skiprows=3, usecols=[9, 14, 15, 37])) # left then right insole forces        \n","                # load starting time\n","                met_raw = np.array(pd.read_excel(data_dir+subj+'\\\\'+filename, usecols=[4], header=None))\n","                starting_time = met_raw[0:2]\n","                test = str(starting_time[0][0]) + ' ' + str(starting_time[1][0])\n","                test_dt = datetime.datetime.strptime(test, '%m/%d/%Y %I:%M:%S %p')    \n","                met_len, cols = met_array.shape\n","                cond_indeces = list(np.arange(1,cond_len+1)) #[1,2,3,4,5,6,7,8,9,10,11,12]\n","                start_inds = ['start 01','start 02','start 03','start 04','start 05','start 06','start 07','start 08','start 09','start 10','start 11','start 12','start 13', 'start 14', 'start 15', 'start 16']\n","                start_inds = start_inds[:cond_len]\n","                hr_int_mat = np.zeros((cond_len, len_cond_s))\n","                met_int_mat = np.zeros((cond_len, len_cond_s))\n","                start_stamps = []\n","                met_inds = []\n","                met_start_inds = np.zeros(cond_len, dtype=int)\n","                hr = np.zeros(cond_len) # first value in each col is length to use of this placeholder vector\n","                met_2mins = np.zeros(cond_len)\n","                met_vals = np.zeros((cond_len,1))\n","                for cnd in range(cond_len): # adding to check for conditions out of order\n","                    for i in range(met_len):\n","                        if met_array[i,-1] == cond_indeces[cnd]:#in cond_indeces:\n","                            met_inds.append(i)\n","                        if met_array[i,-1] == start_inds[cnd]:#in start_inds:\n","                            start_ind = start_inds.index(met_array[i,-1])\n","                            met_start_inds[start_ind] = i\n","                        \n","                # take the average of vco2 and vo2 over the num of avg_breaths, then find W from met eq\n","                for i, ind in enumerate(met_inds):\n","                    end_time = met_array[ind,0]\n","                    start_ind = ind\n","                    start_time = end_time\n","                    #while (start_time.minute != end_time.minute-1) or (start_time.second > end_time.second): # want 1 minute less than end_time\n","                    while (start_time.hour*3600 + start_time.minute*60 + start_time.second + end_time_len) > (end_time.hour*3600 + end_time.minute*60 + end_time.second):\n","                        start_ind -= 1 # decrement\n","                        start_time = met_array[start_ind,0]                    \n","                    vo2 = np.mean(met_array[start_ind:ind,1])\n","                    vco2 = np.mean(met_array[start_ind:ind,2])\n","                    met_vals[i] = (vo2*16.48 + vco2*4.48)/60.0 # BROCKWAY\n","                    \n","                    if add_heartrate: # store values in the hr\n","                        hr[i] = np.mean(met_array[start_ind:ind,-2])\n","                        \n","                for i, ind in enumerate(met_start_inds):\n","                    start_time = met_array[ind,0]\n","                    end_time = start_time\n","                    end_ind = ind\n","                    while((start_time.hour*3600 + start_time.minute*60 + start_time.second + 120) > (end_time.hour*3600 + end_time.minute*60 + end_time.second)): # til end _time is 2 mins more than start\n","                        end_ind += 1 # increment\n","                        end_time = met_array[end_ind,0]\n","                    # get the c02, v02 values for those times, compute met\n","                    vo2 = met_array[ind:end_ind,1]\n","                    vco2 = met_array[ind:end_ind,2]\n","                    met_vec = (vo2*16.48 + vco2*4.48)/60.0 # BROCKWAY\n","                    tvec = np.zeros(end_ind-ind)\n","                    for cnt,j in enumerate(range(ind,end_ind)):\n","                        time_stamp = met_array[j,0]\n","                        tvec[cnt] = time_stamp.hour*3600 + time_stamp.minute*60 + time_stamp.second\n","                    tvec = tvec - tvec[0] + 1 # offset the time so starts at 1\n","                    met_est, y_bar, mean_squared_err = metabolic_rate_estimation(tvec, met_vec)\n","                    met_2mins[i] = met_est\n","                    \n","                    # computing the vectors for each 6 minute interval of interpolated estimates\n","                    end_time2 = start_time\n","                    dt_s = start_time.hour*3600 + start_time.minute*60 + start_time.second\n","                    start_stamps.append(test_dt + datetime.timedelta(0, dt_s)) # adding time to datetime starting time\n","                    end_ind2 = ind\n","                    while((start_time.hour*3600 + start_time.minute*60 + start_time.second + len_cond_s) > (end_time2.hour*3600 + end_time2.minute*60 + end_time2.second)): # til end _time is X mins more than start\n","                        end_ind2 += 1 # increment\n","                        end_time2 = met_array[end_ind2,0]\n","                    # get the c02, v02 values for those times, compute met\n","                    vo2 = met_array[ind:end_ind2,1]\n","                    vco2 = met_array[ind:end_ind2,2]\n","                    hr_vec5 = met_array[ind:end_ind2,3]\n","                    met_vec5 = (vo2*16.48 + vco2*4.48)/60.0 # BROCKWAY\n","                    tvec5 = np.zeros(end_ind2-ind)\n","                    for cnt,j in enumerate(range(ind,end_ind2)):\n","                        time_stamp = met_array[j,0]\n","                        tvec5[cnt] = time_stamp.hour*3600 + time_stamp.minute*60 + time_stamp.second\n","                    tvec5 = tvec5 - tvec5[0] + 1 # offset the time so starts at 1\n","                    tvec5_int = np.arange(1,len_cond_s+1)\n","                    met_vec5_int = np.interp(np.array(tvec5_int,dtype='float64'), np.array(tvec5,dtype='float64'),np.array(met_vec5,dtype='float64'))\n","                    hr_vec5_int = np.interp(np.array(tvec5_int,dtype='float64'), np.array(tvec5,dtype='float64'),np.array(hr_vec5,dtype='float64'))\n","                    hr_int_mat[i,:] = hr_vec5_int\n","                    met_int_mat[i,:] = met_vec5_int\n","                    \n","                for i,ind in enumerate(met_start_inds):\n","                    stop_ind = met_inds[i]\n","                    t_mat.append(met_array[ind:stop_ind,0])\n","                    met_mat_temp = (met_array[ind:stop_ind,1]*16.48 + met_array[ind:stop_ind,2]*4.48)/60.0\n","                    met_mat.append(met_mat_temp)\n","                    hr_mat.append(met_array[ind:stop_ind,-2])\n","    if visualize:\n","        print('Metabolic values (Watts):', met_vals)\n","        print('2 min. Met values: ',met_2mins)\n","        if add_heartrate:\n","            print('HR: ', hr)\n","    return met_vals, hr, met_2mins, hr_int_mat, met_int_mat, start_stamps, t_mat, met_mat, hr_mat\n","\n","def load_constants(cur_dir, subjects):\n","    code_files = os.listdir(cur_dir)\n","    for fnm in code_files:\n","        if len(fnm) >= 6:\n","            if fnm[-6:] == 'rt.csv':\n","                subj_data = np.array(pd.read_csv(cur_dir+'\\\\'+fnm, sep=\",\", skiprows=0, usecols=[1,2]))\n","                num_subj, cols = subj_data.shape\n","                if len(subjects) <= num_subj:\n","                    masses = subj_data[:len(subjects),1]\n","                    heights = subj_data[:len(subjects),0]\n","                    return masses, heights\n","                else:\n","                    print(\"Trying to pull more subjects of data from code/subjects.csv than are there...\")\n","                    return\n","    print(\"No subjects.cvs file in the code folder...\")\n","    \n","# split the data into a subset of features based on text input and a list of features    \n","def loadSignals(signals, data_list, num_bins = 30):\n","    labels = data_list[0]\n","    signal_list = signals.split() # create individual list of features to include\n","    num_constants = data_list[6]\n","    if \"time\" in labels: # weird convention I'm using...\n","        num_constants += 1\n","\n","    signal_ind = [] # store indeces of signals here\n","    new_labels = [] # store included signal labels here\n","    for signal in signal_list:\n","        for i, label in enumerate(labels): # go through labels looking for partial matches\n","            if len(signal) <= len(label): # can check for subset without error\n","                if signal == label[:len(signal)]: # includes subset and add indeces\n","                    new_labels.append(label)\n","                    if i < num_constants: # include column number\n","                        signal_ind.append(i)\n","                    else:\n","                        start_ind = num_constants + (i - num_constants) * num_bins\n","                        end_ind = num_constants + (i - num_constants + 1) * num_bins\n","                        for ind in range(start_ind, end_ind):\n","                            signal_ind.append(ind)\n","    signal_index_final = list(signal_ind)\n","\n","    return signal_index_final, new_labels\n","\n","def readDataCSV(csvfile):\n","    holdout_cond_raw = [] # init in case not defined\n","    with open(csvfile, \"rt\") as f:\n","        reader = csv.reader(f, delimiter=\"\\t\")\n","        for i, line in enumerate(reader):\n","            if i == 0:\n","                features_raw = line\n","            elif i == 1:\n","                seed_list_raw = line\n","            elif i == 2:\n","                sym_list_raw = line\n","            elif i == 3:\n","                new_dir_raw = line\n","            elif i == 4:\n","                subjs_raw = line\n","            elif i == 5:\n","                conds_raw = line\n","            elif i == 6:\n","                const_raw = line\n","            elif i == 7:\n","                norm_raw = line\n","            elif i == 8:\n","                dataset_raw = line\n","            elif i == 9:\n","                freq_raw = line\n","            elif i == 10:\n","                subj_raw = line\n","            elif i == 11:\n","                cond_raw = line\n","            elif i == 12:\n","                holdout_cond_raw = line\n","                \n","    features = features_raw[0].split(',')\n","    seed_list = seed_list_raw[0].split(',')\n","    sym_list = sym_list_raw[0].split(',')\n","    new_dir_temp = new_dir_raw[0].split(',')\n","    new_dir = new_dir_temp[1]\n","    subjs_temp = subjs_raw[0].split(',')\n","    subjs = int(subjs_temp[1])\n","    conds_temp = conds_raw[0].split(',')\n","    conds = int(conds_temp[1])\n","    const_temp = const_raw[0].split(',')\n","    const = int(const_temp[1])\n","    norm_temp = norm_raw[0].split(',')\n","    norm = int(norm_temp[1])\n","    dataset_temp = dataset_raw[0].split(',')\n","    dataset = dataset_temp[0]\n","    freq = freq_raw[0].split(',')\n","    subj_list = subj_raw[0].split(',')\n","    cond_list = cond_raw[0].split(',')\n","    try:\n","        if holdout_cond_raw != []:\n","            holdout_cond = holdout_cond_raw[0].split(',')\n","            while '' in holdout_cond:\n","                holdout_cond.remove('')\n","        else:\n","            holdout_cond = holdout_cond_raw\n","    except:\n","        holdout_cond = []\n","\n","    while '' in features:\n","        features.remove('')\n","    while '' in seed_list:\n","        seed_list.remove('')\n","    while '' in sym_list:\n","        sym_list.remove('')\n","    while '' in freq:\n","        freq.remove('')\n","    while '' in subj_list:\n","        subj_list.remove('')\n","    while '' in cond_list:\n","        cond_list.remove('')\n","    \n","    data_list = [features, seed_list, sym_list, new_dir, subjs, conds, const, norm, dataset, freq, subj_list, cond_list, holdout_cond]\n","\n","    return data_list\n","    \n","def loadFiles(path, avg_num_steps=1):\n","    os.chdir(path)\n","    x_data = genfromtxt('x.csv', delimiter=',')\n","    y_data = genfromtxt('y.csv', delimiter=',')\n","    #print(y_data, type(y_data))\n","    if y_data.size != 1: # added this line in!!!\n","        y_data = np.reshape(y_data,(y_data.shape[0],-1)) #(num,) -> (num,1) so vstack doesn't throw error\n","    if avg_num_steps != 1: # average N steps and chop off remainder\n","        x_data = groupedAvg(x_data,avg_num_steps)\n","        y_data = groupedAvg(y_data,avg_num_steps)\n","    return x_data, y_data\n","\n","# downsample or upsample (by randomly redrawing samples) number of gaits from each condition until all the same\n","def fixSamples(x, number_gaits, first_cycles):\n","    num_cycles = x.shape[0]\n","    if num_cycles == number_gaits:\n","        return x\n","    elif num_cycles < number_gaits: # randomly sample from gaits\n","        num_needed = number_gaits - num_cycles\n","        s_range = range(0,num_cycles)\n","        sample_inds = np.random.choice(s_range, num_needed, True)\n","        for ind in sample_inds:\n","            x = np.append(x, [x[ind,:]], axis=0)\n","        \n","    else: # take the last X number of gaits\n","        if first_cycles:\n","            x = x[:number_gaits,:]\n","        else:\n","            x = x[num_cycles-number_gaits:,:]\n","    return x\n","\n","# find std across all bins of all relevant params and eliminate cycles that don't lie within threshold params            \n","def elimOutliers(x, run_data, labels, outlier_std, num_pts_outside, outlier_feat_list, num_constants, num_bins=30):\n","    if \"time\" in labels:\n","        num_constants += 1 # skip one more space to not filter time\n","        \n","    cycles = x.shape[0]\n","    cycles_to_elim = [] # list of indeces to elim\n","    \n","    for i, param in enumerate(outlier_feat_list):\n","        if param in labels:\n","            ind = labels.index(param) - num_constants\n","            start = num_constants+ind*num_bins\n","            stop = start+num_bins\n","            std_profile = np.std(x[:,start:stop], axis=0) # std across all bins\n","            mean_profile = np.mean(x[:,start:stop], axis=0)\n","            \n","            for j in range(cycles): # go through each gait cycle and check if within threshold\n","                below_thresh = (x[j,start:stop] <= (mean_profile - outlier_std*std_profile)) \n","                above_thresh = (x[j,start:stop] >= (mean_profile + outlier_std*std_profile))\n","                #print(below_thresh)\n","                num_outside_thresh = sum(below_thresh) + sum(above_thresh)\n","    # elim these number cycles\n","    cycles_to_elim.sort()\n","    x = np.delete(x, cycles_to_elim, 0)\n","                  \n","    return x\n","\n","# find closest X number of gait cycles to average measurement for gyro_shank_z_L data\n","def simpleElimOutliers(x, gait_times, num_gait_to_keep, time_dif = 0.7):\n","    cycles_to_elim = []\n","    mean_time = np.mean(gait_times)\n","    for i, time in enumerate(gait_times):\n","        if (time < time_dif*mean_time) or (time > (1 + (1-time_dif))*mean_time):\n","            cycles_to_elim.append(i)\n","    cycles_to_elim.sort()\n","    x = np.delete(x, cycles_to_elim, 0)  \n","    if len(cycles_to_elim) > 0:\n","        print(\"Removed \", len(cycles_to_elim),\" gait cycles for being too short or long.\")\n","    \n","    num_gaits,_ = x.shape\n","    x_feat = x[:,62:92]\n","    x_feat_mean = np.mean(x_feat, axis=0)\n","    x_sum_abs_diff = np.sum(abs(x_feat - np.expand_dims(x_feat_mean,axis=0)), axis=1)\n","    if num_gaits > num_gait_to_keep:\n","        x_sum_abs_diff.sort()\n","        x_thresh = x_sum_abs_diff[num_gait_to_keep-1]\n","        cycles_to_elim = []\n","        for i in range(num_gaits):\n","            if x_sum_abs_diff[i] > x_thresh:\n","                cycles_to_elim.append(i)\n","\n","        cycles_to_elim.sort()\n","        x = np.delete(x, cycles_to_elim, 0)\n","        return x\n","    else:\n","        print(\"Not enough gait cycles to remove any...\")\n","        return x\n","\n","# input data is (examples x features*bins), plotting multiple binned examples\n","# not for plotting the constants features\n","def plotBinDataRetVec(labels_to_plot, start_ex, num_examples, data, labels, num_bins, num_constants):\n","    if \"time\" in labels:\n","        num_constants += 1 # skip one more space to not filter time\n","    for label in labels_to_plot:\n","        for example in range(num_examples):\n","            ind = labels.index(label)\n","            bin_ind = (ind - num_constants)*num_bins + num_constants\n","            plt.plot(data[start_ex + example, bin_ind:bin_ind+num_bins])\n","    plt.xlabel('Percent of gait cycle')\n","    plt.ylabel('Feature value')\n","    plt.xticks([0, 14, 29], ['0','50','100'])\n","    plt.show()\n","    return np.mean(data[start_ex:start_ex+num_examples, bin_ind:bin_ind+num_bins],axis=0)"]}]}